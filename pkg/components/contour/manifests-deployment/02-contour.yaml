apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  labels:
    app: contour
  name: contour
  namespace: heptio-contour
spec:
  selector:
    matchLabels:
      app: contour
  replicas: 2
  template:
    metadata:
      labels:
        app: contour
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "8002"
        # XXX: Modified manually to scrape envoy metrics on prometheus format
        prometheus.io/path: "/stats/prometheus"
        prometheus.io/format: "prometheus"
    spec:
      containers:
      # XXX: We use a custom docker image to fix envoy consuming loads of RAM
      # when running under high traffic.
      #
      # The image is based on contour 0.12-dirty (i.e. before v0.12 was
      # released) and it just contains this simple patch:
      # https://github.com/johananl/contour/commit/bece50124c5398ead56e24e4bb3dcd3a2ff51035
      #
      # The issue is that connections stay open until the client closes them (if
      # he does it at all). Under high traffic load, this ends with tons of open
      # connections and LOT of RAM usage. In fact, in the scenarios we've
      # observed, the RAM usage as well as the opened connections were constantly
      # inreasing until envoy run out of RAM and crashes.
      #
      # This behaviour of not closing open connections is the default behaviour
      # of the envoy http connection manager and there is is a parameter on
      # envoy to tune when an idle connection will be closed (idle_timeout):
      #
      # https://www.envoyproxy.io/docs/envoy/latest/api-v2/config/filter/network/http_connection_manager/v2/http_connection_manager.proto.html?highlight=idle_timeout
      #
      # When setting this parameter on envoy, connections are closed and the RAM
      # usage problem is solved immediately.
      #
      # However, as contour does not expose a way to tune this envoy parameter,
      # the patch just mentioned hardcodes the idle_timeout param with a value
      # of 60s.
      #
      # tl;dr: With this patch, the mem problem is solved immediately and
      # mem remains at constant usage even under heavy traffic load.
      #
      # We already talked to upstream and the proper fix will be discussed on a
      # github issue.
      #
      - image: quay.io/kinvolk/contour:idle_timeout
        imagePullPolicy: Always
        name: contour
        command: ["contour"]
        args: ["serve", "--incluster"]
      - image: docker.io/envoyproxy/envoy:v1.9.1
        name: envoy
        ports:
        - containerPort: 8080
          name: http
        - containerPort: 8443
          name: https
        command: ["envoy"]
        args:
        - --config-path /config/contour.json
        - --service-cluster cluster0
        - --service-node node0
        - --log-level info
        readinessProbe:
          httpGet:
            path: /healthz
            port: 8002
          initialDelaySeconds: 3
          periodSeconds: 3
        volumeMounts:
        - name: contour-config
          mountPath: /config
        lifecycle:
          preStop:
            exec:
              command: ["wget", "-qO-", "http://localhost:9001/healthcheck/fail"] 
      initContainers:
      # XXX: See comment above for the reason for the custom docker image
      - image: quay.io/kinvolk/contour:idle_timeout
        imagePullPolicy: Always
        name: envoy-initconfig
        command: ["contour"]
        args:
        - bootstrap
        # Uncomment the statsd-enable to enable statsd metrics
        #- --statsd-enable
        # Uncomment to set a custom stats emission address and port
        #- --stats-address=0.0.0.0
        #- --stats-port=8002
        - /config/contour.json
        volumeMounts:
        - name: contour-config
          mountPath: /config
      volumes:
      - name: contour-config
        emptyDir: {}
      dnsPolicy: ClusterFirst
      serviceAccountName: contour
      terminationGracePeriodSeconds: 30
      # The affinity stanza below tells Kubernetes to try hard not to place 2 of
      # these pods on the same node.
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchLabels:
                  app: contour
              topologyKey: kubernetes.io/hostname
---
